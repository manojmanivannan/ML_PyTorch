{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_1 = pd.read_csv('cox_call_metrics_26_31_jan_byNE_target.csv', parse_dates=['time'])\n",
    "# pm_data_2 = pd.read_csv('cox_call_metrics_31-06_Feb_5min_byNE_target.csv', parse_dates=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    'time',\n",
    "    'network_element_b_id.name',\n",
    "    'common_average_connect_time',\n",
    "    'common_average_disconnect_time',\n",
    "    'common_call_failed_disconnect_volume',\n",
    "    'common_call_failed_call_setup_volume',\n",
    "    'common_call_failed_answered_call_volume',\n",
    "    'common_call_ner',\n",
    "    'common_call_asr'\n",
    "]\n",
    "pm_data_1 = pm_data_1[selected_columns]\n",
    "# pm_data_2 = pm_data_2[selected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pm_data_1.time.nunique()/12/24==5.0 # 5 days worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_1['network_element_b_id.name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = pm_data_1.set_index(['time', 'network_element_b_id.name'])\n",
    "pm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the global minimum and maximum dates across the entire DataFrame\n",
    "global_min_date = pm_data.index.get_level_values(0).min()  # Adjust the level if necessary\n",
    "global_max_date = pm_data.index.get_level_values(0).max()  # Adjust the level if necessary\n",
    "\n",
    "# Create a complete DateTimeIndex based on the global min and max dates with 5-minute frequency\n",
    "global_complete_index = pd.date_range(start=global_min_date, end=global_max_date, freq='5T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(global_min_date)\n",
    "print(global_max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex for each mo_name\n",
    "reindexed_dfs = []\n",
    "for mo_name in pm_data.index.get_level_values('network_element_b_id.name').unique():\n",
    "    # Select data for the current mo_name\n",
    "    sub_df = pm_data.xs(mo_name, level='network_element_b_id.name')\n",
    "\n",
    "    # Reindex the subset DataFrame using the global index and restore mo_name in the index\n",
    "    sub_df_reindexed = sub_df.reindex(global_complete_index)\n",
    "    sub_df_reindexed['network_element_b_id.name'] = mo_name\n",
    "    sub_df_reindexed.set_index('network_element_b_id.name', append=True, inplace=True)\n",
    "\n",
    "    reindexed_dfs.append(sub_df_reindexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the reindexed DataFrames\n",
    "pm_data = pd.concat(reindexed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data = pm_data.reset_index()\n",
    "pm_data.rename(columns = {'level_0':'time'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     pm_data.drop('time',axis=1)\\\n",
    "#         .groupby('network_element_b_id.name').sum()\\\n",
    "#             .sort_values(by='common_call_failed_answered_call_volume', ascending=False)[['common_call_failed_answered_call_volume']]\\\n",
    "#                 .to_string()\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data.to_csv(\"cox_data_aligned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sweetviz.analyze([pm_data, \"DataFrame\"])\n",
    "report.show_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_filtered = pm_data.loc[pm_data['network_element_b_id.name']=='DUKEVMSIP1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Metrics to be plotted\n",
    "metrics = selected_columns\n",
    "metrics.remove('time')\n",
    "metrics.remove('network_element_b_id.name')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(20, 20), sharex=True)\n",
    "\n",
    "# Loop through metrics and plot\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].plot(pm_data_filtered['time'], pm_data_filtered[metric], label=f'{metric} Data')\n",
    "    #axes[i].scatter(anomaly_times, pm_data[metric].iloc[original_anomalous_indexes], color='red', label='Anomaly')\n",
    "    axes[i].set_title(f'{metric}',fontsize=8)\n",
    "    axes[i].set_ylabel(metric ,fontsize=8)\n",
    "\n",
    "\n",
    "# Label the shared x-axis\n",
    "axes[-1].set_xlabel('Time')\n",
    "ticks_to_use = pm_data_filtered['time'][::30]  # Choose every 10th time point\n",
    "plt.xticks(ticks_to_use)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format the dates on x-axis\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "plt.gca().xaxis.set_major_formatter(date_format)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate_of_non_null_over_null(df):\n",
    "    # Calculate non-null counts for each group\n",
    "    non_null_counts = df.groupby('network_element_b_id.name').count()\n",
    "    \n",
    "    # Calculate null counts for each group\n",
    "    null_counts = df.groupby('network_element_b_id.name').apply(lambda x: x.isnull().sum())\n",
    "    \n",
    "    # Calculate the rate of non-null to null values\n",
    "    rate = non_null_counts / null_counts\n",
    "    \n",
    "    # Replace infinite values with NaN (occurs when dividing by zero)\n",
    "    rate.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return rate\n",
    "\n",
    "rate_df = calculate_rate_of_non_null_over_null(pm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_df.sort_values(by='common_call_failed_answered_call_volume', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns for ML training\n",
    "pm_data_ml = pm_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Missingness Indicators\n",
    "for column in pm_data_ml.columns:\n",
    "    pm_data_ml[column + '_missing'] = pm_data_ml[column].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_ml_filtered = pm_data_ml.loc[pm_data_ml['network_element_b_id.name']=='STI-AS-WEST2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to be plotted\n",
    "metrics = ['common_average_connect_time',\n",
    "           'common_call_asr', \n",
    "           'common_call_asr_missing'\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(20, 6), sharex=True)\n",
    "\n",
    "# Loop through metrics and plot\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].plot(pm_data_ml_filtered['time'], pm_data_ml_filtered[metric], label=f'{metric} Data')\n",
    "    #axes[i].scatter(anomaly_times, pm_data[metric].iloc[original_anomalous_indexes], color='red', label='Anomaly')\n",
    "    axes[i].set_title(f'{metric}',fontsize=8)\n",
    "    axes[i].set_ylabel(metric ,fontsize=8)\n",
    "\n",
    "\n",
    "# Label the shared x-axis\n",
    "axes[-1].set_xlabel('Time')\n",
    "ticks_to_use = pm_data_ml_filtered['time'][::1000]  # Choose every 10th time point\n",
    "plt.xticks(ticks_to_use)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_ml = pm_data_ml.drop(['time_missing', 'network_element_b_id.name_missing'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data, fill nulls with zeros \n",
    "pm_data_ml.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['common_average_connect_time',\n",
    "                                'common_average_disconnect_time',\n",
    "                                'common_call_failed_disconnect_volume',\n",
    "                                'common_call_failed_call_setup_volume',\n",
    "                                'common_call_failed_answered_call_volume', \n",
    "                                'common_call_ner',\n",
    "                                'common_call_asr', \n",
    "                                'common_average_connect_time_missing',\n",
    "                                'common_average_disconnect_time_missing',\n",
    "                                'common_call_failed_disconnect_volume_missing',\n",
    "                                'common_call_failed_call_setup_volume_missing',\n",
    "                                'common_call_failed_answered_call_volume_missing',\n",
    "                                'common_call_ner_missing', \n",
    "                                'common_call_asr_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create windowed sequences\n",
    "def create_windows(data, window_size, stride=1):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "    \n",
    "# Calculate overall mean and std\n",
    "all_metrics_data = pm_data_ml[selected_columns].to_numpy()\n",
    "\n",
    "overall_mean, overall_std = np.mean(all_metrics_data, axis=0), np.std(all_metrics_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store windowed and normalized data for all cell_ids\n",
    "X_list, y_list = [], []\n",
    "\n",
    "window_size = 144\n",
    "\n",
    "# Group data by mo_name and loop over each group\n",
    "for mo_name, group in pm_data_ml.groupby('network_element_b_id.name'):\n",
    "    # Assuming the DataFrame is already sorted by time, if not sort it here\n",
    "    group = group.sort_values(by='time')\n",
    "    \n",
    "    # Select only the columns containing the metrics\n",
    "    metrics_data = group[selected_columns].to_numpy()\n",
    "    \n",
    "    # Normalize the data using the overall mean and std\n",
    "    metrics_data_normalized = (metrics_data - overall_mean) / overall_std\n",
    "    \n",
    "    # Create windowed sequences\n",
    "    X_window, y_window = create_windows(metrics_data_normalized, window_size, stride=3)\n",
    "    \n",
    "    # Append to the list\n",
    "    X_list.append(X_window)\n",
    "    y_list.append(y_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the windowed data\n",
    "X_batched = np.concatenate(X_list, axis=0)\n",
    "y_batched = np.concatenate(y_list, axis=0)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_batched_tensor = torch.tensor(X_batched, dtype=torch.float32).view(-1, window_size, len(selected_columns))\n",
    "y_batched_tensor = torch.tensor(y_batched, dtype=torch.float32).view(-1, len(selected_columns))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    X_batched_tensor = X_batched_tensor.to(device)\n",
    "    y_batched_tensor = y_batched_tensor.to(device)\n",
    "    print(\"Tensors moved to GPU\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_batched_tensor shape:\", X_batched_tensor.shape)\n",
    "print(\"y_batched_tensor shape:\", y_batched_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_size = int(0.8 * len(X_batched_tensor))\n",
    "val_size = len(X_batched_tensor) - train_size\n",
    "\n",
    "X_train = X_batched_tensor[:train_size]\n",
    "y_train = y_batched_tensor[:train_size]\n",
    "\n",
    "X_val = X_batched_tensor[train_size:]\n",
    "y_val = y_batched_tensor[train_size:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the Transformer Model\n",
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerTimeSeriesModel, self).__init__()\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Expected input dimension (L, N, E) - Length, Batch size, Embedding\n",
    "        x = x.permute(1, 0, 2)  # permute to fit transformer's expected input shape\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.linear_out(x)\n",
    "        x = x.permute(1, 0, 2)  # permute back to original shape\n",
    "        return x[:, -1, :]  # return only the last prediction for each sequence\n",
    "\n",
    "# Model Hyperparameters\n",
    "#d_model = 6  # Dimension of the input vector, we have 6 metrics\n",
    "#nhead = 2  # Number of heads in the multihead attention models\n",
    "#num_layers = 3  # Number of sub-encoder-layers in the transformer encoder\n",
    "#dim_feedforward = 128  # Dimension of the feedforward network model\n",
    "#batch_size = 64  # your batch size\n",
    "\n",
    "# Initialize the Model, Loss, and Optimizer\n",
    "#model = TransformerTimeSeriesModel(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "# Reduced model hyperparameters\n",
    "d_model = len(selected_columns)\n",
    "nhead = 2  # reduced from 2\n",
    "num_layers = 2  # reduced from 3\n",
    "dim_feedforward = 64  # reduced from 128\n",
    "#batch_size = 32  # reduced from 64\n",
    "\n",
    "# Initialize the reduced Model, Loss, and Optimizer\n",
    "model = TransformerTimeSeriesModel(d_model, nhead, num_layers, dim_feedforward)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Optional: Gradient clipping\n",
    "clip_value = 1.0\n",
    "        \n",
    "        \n",
    "#Send model to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    print(\"Model moved to GPU\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "import torch\n",
    "import copy\n",
    "import os\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 2\n",
    "best_val_loss = float('inf')\n",
    "num_epochs_no_improve = 0\n",
    "best_model = None\n",
    "model_save_path = 'best_model.pth'  # Define the path where you want to save the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_batch)\n",
    "        loss = criterion(outputs, y_train_batch)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_loss += criterion(val_outputs, y_val_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {loss.item()}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Checkpoint and Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        num_epochs_no_improve = 0\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model, model_save_path)  # Save the best model state to disk\n",
    "    else:\n",
    "        num_epochs_no_improve += 1\n",
    "\n",
    "    if num_epochs_no_improve == patience:\n",
    "        print(f\"Early stopping triggered. Validation loss did not decrease for {patience} consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Load the best model for use\n",
    "model.load_state_dict(torch.load(model_save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              'window_size': window_size,\n",
    "              'overall_mean': overall_mean,\n",
    "              'overall_std': overall_std,\n",
    "              # ... any other data\n",
    "             }\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint_ccable2_transf_v1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_reconstruction_errors = []\n",
    "\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_batch)\n",
    "        reconstruction_error = ((y_batch.cpu() - y_pred.cpu()) ** 2).mean(dim=1).numpy()\n",
    "        all_reconstruction_errors.extend(reconstruction_error)\n",
    "\n",
    "all_reconstruction_errors = np.array(all_reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all_reconstruction_errors.npy', all_reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reconstruction_errors = np.load('all_reconstruction_errors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins \n",
    "bin_ranges = pd.cut(all_reconstruction_errors, bins=50)\n",
    "\n",
    "# Count the frequency of each bin\n",
    "bin_counts = bin_ranges.value_counts().sort_index()\n",
    "\n",
    "# Create a Pandas DataFrame for better visualization\n",
    "table = pd.DataFrame({\n",
    "    'Bin_Range': bin_counts.index.astype(str),\n",
    "    'Frequency': bin_counts.values\n",
    "})\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(all_reconstruction_errors, 99.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_data_filtered = pm_data_ml.loc[pm_data_ml['network_element_b_id.name']=='FLCHVA19DS0-NGSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics_data = pm_data_filtered[selected_columns].to_numpy()\n",
    "\n",
    "# Using `create_windows` function to create windowed sequences\n",
    "new_X_window, new_y_window = create_windows(new_metrics_data, window_size)\n",
    "\n",
    "# Step 4: Normalize the windowed data using original mean and std\n",
    "new_X_window_normalized = (new_X_window - overall_mean) / overall_std\n",
    "new_y_window_normalized = (new_y_window - overall_mean) / overall_std\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "new_X_tensor = torch.tensor(new_X_window_normalized, dtype=torch.float32).view(-1, window_size, 22)\n",
    "new_y_tensor = torch.tensor(new_y_window_normalized, dtype=torch.float32).view(-1, 22)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    new_X_tensor = new_X_tensor.to(device)\n",
    "    new_y_tensor = new_y_tensor.to(device)\n",
    "\n",
    "# Create DataLoader if needed\n",
    "new_dataset = TimeSeriesDataset(new_X_tensor, new_y_tensor)\n",
    "new_loader = DataLoader(new_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "anomalies = []\n",
    "filt_reconstruction_errors = []\n",
    "\n",
    "for batch_idx, (X_batch, y_batch) in enumerate(new_loader):  # Replace val_loader with your test_loader if using new data\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_batch)\n",
    "        reconstruction_error = ((y_batch.cpu() - y_pred.cpu()) ** 2).mean(dim=1).numpy()\n",
    "        \n",
    "        filt_reconstruction_errors.extend(reconstruction_error)\n",
    "        batch_anomalies = reconstruction_error > threshold\n",
    "        anomalies.extend(batch_anomalies)\n",
    "\n",
    "filt_reconstruction_errors = np.array(filt_reconstruction_errors)\n",
    "anomalies = np.array(anomalies)\n",
    "num_anomalies = np.sum(anomalies)\n",
    "print(f\"Number of anomalies: {num_anomalies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we calculate the number of anomalies based on the pre-calculated Anomaly score (reconstruction_errors)\n",
    "anomalies_o = filt_reconstruction_errors > threshold\n",
    "num_anomalies_o = np.sum(anomalies_o)\n",
    "print(f\"Number of anomalies: {num_anomalies_o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "\n",
    "\n",
    "# Prepare anomaly indices (same as previous examples)\n",
    "anomalous_indexes_val = np.where(anomalies_o)[0]\n",
    "original_anomalous_indexes = anomalous_indexes_val + window_size\n",
    "anomaly_times = pm_data_filtered['time'].iloc[original_anomalous_indexes]\n",
    "\n",
    "# Metrics to be plotted\n",
    "metrics = selected_columns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(15, 35), sharex=True)\n",
    "\n",
    "# Loop through metrics and plot\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].plot(pm_data_filtered['time'], pm_data_filtered[metric], label=f'{metric} Data')\n",
    "    #axes[i].scatter(anomaly_times, pm_data[metric].iloc[original_anomalous_indexes], color='red', label='Anomaly')\n",
    "    axes[i].set_title(f'{metric}',fontsize=8)\n",
    "    axes[i].set_ylabel(metric ,fontsize=8)\n",
    "    for anomaly_time in anomaly_times:\n",
    "        axes[i].axvline(x=anomaly_time, color='red', linestyle='--', linewidth=1, label='Anomaly')\n",
    "    #axes[i].legend()\n",
    "\n",
    "\n",
    "# Label the shared x-axis\n",
    "axes[-1].set_xlabel('Time')\n",
    "ticks_to_use = pm_data_filtered['time'][::20]  # Choose every 10th time point\n",
    "plt.xticks(ticks_to_use)\n",
    "plt.xticks(rotation=40)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "\n",
    "checkpoint = torch.load('checkpoint_ccable2_transf_v1.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "loss = checkpoint['loss']\n",
    "window_size = checkpoint['window_size']\n",
    "overall_mean = checkpoint['overall_mean']\n",
    "overall_std = checkpoint['overall_std']\n",
    "\n",
    "#Send model to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    print(\"Model moved to GPU\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('FLCHVA19DS0-NGSS_reconstruction_errors.npy', filt_reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_reconstruction_errors = np.load('FLCHVA19DS0-NGSS_reconstruction_errors.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
